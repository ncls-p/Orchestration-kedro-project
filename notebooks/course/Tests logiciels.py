#!/usr/bin/env python
# coding: utf-8

# Dans l'univers du d√©veloppement logiciel, les tests sont omni-pr√©sents. Ils permettent de v√©rifier que le logiciel ou l'application d√©velopp√©e adopte correctement le comportement attendu, ne produit pas de bugs ou s'int√®gre efficacement dans un environnement existant.
# 
# Mais comment transposer tous ces tests de d√©veloppement logiciel au cas o√π l'on entra√Æne et fait intervenir des mod√®les de Machine Learning ?
# 
# <blockquote><p>üôã <b>Ce que nous allons faire</b></p>
# <ul>
#     <li>Comprendre pourquoi il est important de tester son code et son mod√®le</li>
#     <li>Appliquer les tests usuels de d√©veloppement logiciel</li>
# </ul>
# </blockquote>
# 
# <img src="https://media.giphy.com/media/Rd6sn03ncIklmprvy6/giphy.gif" />

# ## Tester des applications
# 
# Avant de rentrer dans le d√©tails des tests d'algorithmes de Machine Learning, d√©crivons tout d'abord les bonnes pratiques h√©rit√©es du d√©veloppement logiciel.
# 
# ### Tests logiciels
# 
# Dans ce contexte, une suite de tests inclut habituellement trois composantes.
# 
# - Les **tests unitaires**, o√π l'on s'assurer qu'une portion atomique du code fonctionne correctement (par exemple, une fonction). En r√®gle g√©n√©rale, ce sont des tests rapides et faciles √† mettre en place.
# - Les **tests de r√©gression**, o√π l'on doit s'assurer que le d√©veloppement d'une nouvelle fonctionnalit√© ne va pas faire survenir un bug d√©j√† rencontr√© par le pass√©.
# - Les **tests d'int√©gration**, o√π on cherche √† voir si la fonctionnalit√© d√©velopp√©e va √™tre correctement int√©gr√© dans l'application sans g√©n√©rer des erreurs dues √† son interaction avec d'autres composantes. Ces erreurs sont en pratique plus difficiles √† pr√©venir, d'o√π la difficult√© de construire des tests d'int√©gration efficaces.
# 
# Dans les faits, les bonnes pratiques n√©cessitent de suivre plusieurs conventions. En travail collaboratif, notamment avec `git`, les r√®gles de base suivantes sont appliqu√©es.
# 
# - Ne **jamais fusionner de branches** si les tests ne sont pas valides.
# - **Toujours √©crire des tests** pour de nouvelles fonctionnalit√©s.
# - Lorsque l'on corrige un bug, **toujours √©crire le test** et l'appliquer sur la correction.
# 
# ### Tests de mod√®les de Machine Learning
# 
# Essayons maintenant de transposer ce que nous venons de voir pour tester les mod√®les de Machine Learning. Une fois un mod√®le de Machine Learning calibr√©, nousz souhaiterions obtenir un rapport d'√©valuation contenant les informations suivantes.
# 
# - Performances avec des m√©triques d√©finies sur des sous-ensembles (`X_test` par exemple).
# - Graphes de validation : courbe PR, courbe ROC, densit√© des classes, courbe de calibration.
# - Audit du mod√®le avec des mod√®les d'interpr√©tabilit√© (PDP, valeurs de Shapley).
# - Sous-population o√π le mod√®le g√©n√®re des faux-positifs ou faux-n√©gatifs avec un fort degr√© de confiance.
# 
# Par ailleurs, on y retrouve √©galement d'autres bonnes pratiques qui s'inscrivent toujours dans une logique de d√©marche de qualit√©.
# 
# - **Toujours sauvegarder** les hyper-param√®tres, sous-√©chantillons utilis√©s et le mod√®le entra√Æn√©.
# - Mettre √† jour un environnement de production avec **un mod√®le aux meilleures performances** ou selon un seuil minimal.
# 
# Face √† ces besoins de tester, nous pouvons voir que calculer des performances sur un sous-√©chantillon ou afficher des courbes n'est pas suffisant pour s'assurer que le mod√®le est ¬´ valide ¬ª. Pour les syst√®mes de Machine Learning, nous devrions effectuer deux m√©thodes en parall√®le.
# 
# - **L'√©valuation de mod√®le**, o√π l'on calcule ses performances, audite son fonctionnement et affiche des courbes.
# - Le **test de mod√®le** o√π l'on d√©veloppe des tests explicites pour v√©rifier que le comportement du mod√®le est bien celui attendu.

# ## Tests unitaires
# 
# Commen√ßons par introduire les tests unitaires avec `pytest`. Il s'agit d'une librairie qui permet de **faciliter la mise en place et l'ex√©cution** des tests de code sous Python. Bien que les tests unitaires puissent √™tre r√©alis√©s *from scratch*, `pytest` am√©liore la productivit√© et apporte des fonctionnalit√©s tr√®s utiles.
# 
# Testons la librairie sur le premier fichier suivant. Nous avons cod√© la fonction `argmax` qui cherche √† obtenir la position du plus grand √©l√©ment d'une liste. Nous codons √©galement la fonction `test_argmax` qui va tester unitairement la fonction `argmax` sur plusieurs exemples : cela refl√®te du comportement attendu de la fonction.

# In[5]:


%%writefile /tmp/pytest_1.py
def argmax(liste):
    if len(liste) == 0:
        return None
    
    idx_max = 0
    value_max = liste[0]
    for i, x in enumerate(liste):
        if x > value_max:
            value_max = x
            idx_max = i
    return idx_max

def test_argmax():
    assert argmax([5, 8, 2, 9, 6, 3]) == 3
    assert argmax([7]) == 0
    assert argmax([]) == None

# Ex√©cutons le code avec `pytest` en sp√©cifiant le chemin d'acc√®s au fichier.

# In[6]:


!pytest /tmp/pytest_1.py

# En ex√©cutant cette commande, `pytest` effectue une **d√©couverte automatique** des tests.
# 
# - Il va d'abord rechercher tous les fichiers dont le nom commence par `test*` si on lui fournit un dossier.
# - Pour chaque classe/fonction du fichier, si l'objet commence par `test*`, alors ce dernier sera instanci√© (dans le cas d'une fonction) et les fonctions seront ex√©cut√©es (pour les deux).
# 
# Cette d√©couverte des tests permet de simplifier la mise en place des tests : plus besoin de sp√©cifier tous les tests dans un fichier, qui lui-m√™me effectue des importations. Nous pouvons imaginer que pour chaque *module*, il y ait un fichier `test.py` qui regroupe tous les tests unitaires li√©s √† ce module. De mani√®re g√©n√©rale, il est plus appropri√© de cr√©er un fichier sp√©cifique pour les tests unitaires plut√¥t que de les ins√©rer dans le code qui fournit la logique √† l'application.
# 
# C'est de cette mani√®re que `pytest` ex√©cute naturellement la fonction `test_argmax` sans avoir eu besoin de la sp√©cifier comme argument. Dans certains cas, nous pouvons √™tre amen√© √† √©viter volontairement l'ex√©cution d'une fonction. Dans ce cas, il suffit d'ajouter le d√©corateur `pytest.mark.skip`.

# In[8]:


%%writefile /tmp/pytest_1.py
import pytest

def argmax(liste):
    if len(liste) == 0:
        return None
    
    idx_max = 0
    value_max = liste[0]
    for i, x in enumerate(liste):
        if x > value_max:
            value_max = x
            idx_max = i
    return idx_max

@pytest.mark.skip
def test_argmax():
    assert argmax([5, 8, 2, 9, 6, 3]) == 3
    assert argmax([7]) == 0
    assert argmax([]) == None

# In[9]:


!pytest /tmp/pytest_1.py

# Comme nous pouvons le voir, 100% des tests ont r√©ussi car le seul test pr√©sent a √©t√© ignor√© (*skipped*). Voyons maintenant un autre fichier Python dont le test unitaire va volontairement g√©n√©rer une erreur.

# In[10]:


%%writefile /tmp/pytest_2.py
def argmin(liste):
    if len(liste) == 0:
        return None
    
    idx_min = 0
    value_min = liste[0]
    for i, x in enumerate(liste):
        if x < value_min:
            value_min = x
            idx_min = i + 1
    return idx_min

def test_argmin():
    assert argmin([5, 8, 2, 9, 6, 3]) == 2
    assert argmin([7]) == 0
    assert argmin([]) == None

# In[11]:


!pytest /tmp/pytest_2.py

# D'apr√®s la sortie g√©n√©r√©e par `pytest`, les tests du fichier `/tmp/pytest_2.py` ont √©chou√©s. Si l'on regarde en d√©taille l'ex√©cution de `test_argmin`, nous avons un `assert 3 == 2`, ce qui signifie que notre test unitaire a √©chou√©. Corrigeons la fonction `argmin` et ajoutons la fonction `argmax` avec son test unitaire associ√©.

# In[12]:


%%writefile /tmp/pytest_2.py
def argmin(liste):
    if len(liste) == 0:
        return None
    
    idx_min = 0
    value_min = liste[0]
    for i, x in enumerate(liste):
        if x < value_min:
            value_min = x
            idx_min = i
    return idx_min

def argmax(liste):
    if len(liste) == 0:
        return None
    
    idx_max = 0
    value_max = liste[0]
    for i, x in enumerate(liste):
        if x > value_max:
            value_max = x
            idx_max = i
    return idx_max

def test_argmin():
    assert argmin([5, 8, 2, 9, 6, 3]) == 2
    assert argmin([7]) == 0
    assert argmin([]) == None
    
def test_argmax():
    assert argmax([5, 8, 2, 9, 6, 3]) == 3
    assert argmax([7]) == 0
    assert argmax([]) == None

# In[13]:


!pytest /tmp/pytest_2.py -v

# Le param√®tre `-v` permet d'afficher plus de d√©tails concernant les tests. Puisque deux fonctions sont nomm√©es `test*`, il y a deux tests effectu√©s par `pytest`. Cette option permet d'obtenir un d√©tail pour chaque test cod√©, simplifiant ensuite le d√©boggage de l'application. 
# 
# En pratique, les tests unitaires doivent √™tre ex√©cut√©s une fois les donn√©es envoy√©s vers le d√©p√¥t Git. En revanche, il est d√©conseill√© de les ex√©cuter lors du pre-commit, car ce dernier doit √™tre rapide. Les tests unitaires, notamment ceux incluant des tests pour les mod√®les, peuvent prendre du temps ce qui n'est pas conseill√© pour les pre-commits.
# 
# ### Les fixtures
# 
# Imaginons que l'on souhaite utiliser des donn√©es/param√®tres uniquement pour les tests unitaires. Si l'on regarde bien, les deux fonctions `test_argmin` et `test_argmax` utilisent les m√™mes listes pour tester les deux fonctions. Nous pourrions tout √† fait d√©finir des catalogues de r√©f√©rence pour les tests unitaires qui seront utilis√©s √† chaque fois. C'est √† cela que servent **les fixtures**.
# 
# Regardons le code suivant qui n'utilise pas de fixture. Nous allons simplement cr√©er une liste `test_data` qui sera utilis√©e par les deux fonctions de test.

# In[14]:


%%writefile /tmp/pytest_2.py

# Pas bien !
test_data = [5, 8, 2, 9, 6, 3]

def argmin(liste):
    if len(liste) == 0:
        return None
    
    idx_min = 0
    value_min = liste[0]
    for i, x in enumerate(liste):
        if x < value_min:
            value_min = x
            idx_min = i
    return idx_min

def argmax(liste):
    if len(liste) == 0:
        return None
    
    idx_max = 0
    value_max = liste[0]
    for i, x in enumerate(liste):
        if x > value_max:
            value_max = x
            idx_max = i
    return idx_max

def test_argmin():
    assert argmin(test_data) == 2
    
def test_argmax():
    assert argmax(test_data) == 3

# In[15]:


!pytest /tmp/pytest_2.py -v

# Bien que le test ait fonctionn√©, cela n'est pas une bonne pratique, car nous allons obligatoirement d√©finir cette variable globale en m√©moire √† chaque ex√©cution du code, alors qu'elle n'est utilis√©e que pour les tests unitaires. Dans ce cas de figure, il est pr√©f√©rable de cr√©er des fixtures.
# 
# Les fixtures d√©finissent un environnement dans lequel nous allons pouvoir tester notre code. Dans beaucoup de situations, il nous faut initialiser certaines variables avant de lancer les tests unitaires. Les fixtures sous `pytest` sont des fonctions qui sont utilis√©s comme **param√®tres** des fonctions de tests unitaires.
# 
# Regardons le code suivant.

# In[17]:


%%writefile /tmp/pytest_2.py
import pytest

@pytest.fixture
def test_data():
    return [5, 8, 2, 9, 6, 3]

def argmin(liste):
    if len(liste) == 0:
        return None
    
    idx_min = 0
    value_min = liste[0]
    for i, x in enumerate(liste):
        if x < value_min:
            value_min = x
            idx_min = i
    return idx_min

def argmax(liste):
    if len(liste) == 0:
        return None
    
    idx_max = 0
    value_max = liste[0]
    for i, x in enumerate(liste):
        if x > value_max:
            value_max = x
            idx_max = i
    return idx_max

def test_argmin(test_data):
    assert argmin(test_data) == 2
    
def test_argmax(test_data):
    assert argmax(test_data) == 3 

# Tout d'abord, nous d√©finissons la fonction `test_data` comme fixture √† l'aide du d√©corateur de fonctions `@pytest.fixture`. Cette fonction va renvoyer une liste qui correspond √† la liste de r√©f√©rence pour tester les deux fonctions. Ensuite, dans les fonctions de tests unitaires, nous allons r√©cup√©rer comme param√®tre cette m√™me fonction `test_data`. Mais attention : lorsque l'on ex√©cutera `pytest`, ce dernier va automatiquement remplacer le param√®tre `test_data` (qui est suppos√© √™tre une fonction car fixture) par le r√©sultat de cette fonction.
# 
# <img src="https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/tests4.png" />
# 
# Ainsi, √† chaque ex√©cution de `pytest`, ce sera en r√©alit√© `test_data()` qui sera pass√© comme param√®tre pour les fonctions `test_argmin` et `test_argmax` (et non la fonction `test_data` elle-m√™me). Cette m√©thode permet d'instancier plus efficacement les initialisations pour les tests, sans compromettre le reste du code qui lui n'aura pas besoin des tests dans un environnement de production.
# 
# Ex√©cutons maintenant `pytest`.

# In[18]:


!pytest /tmp/pytest_2.py -v

# Tout a correctement fonctionn√©. L'int√©r√™t de ce syst√®me est de pouvoir ensuite centraliser l'initialisation des variables et des donn√©es pour les tests, √©vitant ainsi les duplicata de codes que l'on conna√Æt d√©j√† bien hors des tests.
# 
# Maintenant que nous avons vu les points essentiels de `pytest`, nous pouvons dor√©navant int√©grer les tests unitaires dans notre projet Kedro. Et un avantage non n√©gligeable est que Kedro supporte nativement `pytest` pour les tests unitaires : il dispose m√™me de la commande `kedro test`. üôÇ

# ## Int√©gration des tests unitaires dans Kedro
# 
# Int√©grons les tests unitaires et du mod√®le dans notre projet Kedro. En regardant la structure du projet, nous pouvons observer le dossier `src/tests` qui contient le fichier `test_run.py`.
This module contains an example test.

Tests should be placed in ``src/tests``, in modules that mirror your
project's structure, and in files named test_*.py. They are simply functions
named ``test_*`` which test a unit of logic.

To run the tests, run ``kedro test`` from the project root directory.
# Pour ex√©cuter proprement les tests avec Kedro, il faut que la structure des fichiers des tests soit identique √† celle utilis√©e dans `src/purchase_predict`. Nous devons donc cr√©er deux dossiers `loading`, `training` et `processing` dans `src/tests/pipelines` pour r√©pliquer l'architecture √† l'identique.
# 
# Commen√ßons par le dossier `loading` qui charge les fichiers CSV depuis Cloud Storage. Au pr√©alable, nous allons installer les d√©pendances de Kedro pour effectuer les tests unitaires (qui contient `pytest` notamment).
pip install src/requirements.txt
# ### Tests sur les nodes
# 
# Avant de d√©velopper nos tests unitaires, cr√©ons sur le bucket Cloud Storage des **donn√©es de tests**. Dans le dossier `primary/` du bucket, nous allons cr√©er un dossier `data-test.csv/`.
# 
# <img src="https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/tests1.png" />
# 
# Ensuite, pour alimenter ce dossier, nous allons copier deux fichiers CSV d√©j√† pr√©sents dans `data.csv/` vers `data-test.csv/`.
# 
# <img src="https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/tests2.png" />
# 
# Habituellement, avec Kedro, nous pouvons effectuer deux s√©ries de tests.
# 
# - Les tests sur les nodes et les fonctions qu'utilisent les nodes. Par exemple, pour l'entra√Ænement, le fichier `nodes.py` contient des fonctions qui ne sont pas directement utilis√©s par les nodes mais qui sont appel√©s par les fonctions des nodes.
# - Les tests sur les pipelines, permettant de les tester en fonction de plusieurs formats d'entr√©e ou sous forme d'ex√©cution partielles.
# 
# Cr√©ons tout d'abord le fichier `test_nodes.py`. Dans le pipeline `loading`, seule la fonction `load_csv_from_bucket` est pr√©sente : nous allons uniquement tester cette derni√®re.
import pandas as pd

from purchase_predict.pipelines.loading.nodes import load_csv_from_bucket

def test_load_csv_from_bucket(project_id, primary_folder):
    df = load_csv_from_bucket(project_id, primary_folder)
    print(df.head())
# Nous d√©finissons la fonction `test_load_csv_from_bucket` avec les m√™mes param√®tres que la fonction `load_csv_from_bucket`.
# 
# > ‚ùì Mais nous n'avons pas d√©fini les fixtures ici ?
# 
# En effet, il faudrait que les param√®tres `project_id` et `primary_folder` soient des fixtures avec des fonctions de m√™me nom. Or, ici, nous n'en avons pas cr√©√©e. Il y a une raison √† cela : plus tard, nous allons √©galement cr√©er un fichier de test pour le pipeline. Pour √©viter des redondances de d√©finitions de fixtures, nous allons d√©finir les fixtures dans un fichier sp√©cifique, qui derri√®re sera automatiquement ex√©cut√© par `pytest`.
# 
# D'apr√®s <a href="https://docs.pytest.org/en/stable/fixture.html#conftest-py-sharing-fixture-functions" target="_blank">la documentation</a> de `pytest` sur les fixtures, nous pouvons les centraliser dans un fichier nomm√© `conftest.py` qui sera automatiquement ex√©cut√© avant les tests unitaires. Nous en cr√©ons un dans le dossier `loading`.
import pytest

@pytest.fixture(scope="module")
def project_id():
    return "<PROJECT_GCP>"

@pytest.fixture(scope="module")
def primary_folder():
    return "<NOM_DU_BUCKET>/primary/data-test.csv"
# L'argument `scope="module"` permet de sp√©cifier que les fixtures seront accessibles √† l'int√©rieur de `purchase_predict`. Il ne reste plus qu'√† lancer les test avec Kedro.
kedro test========================= test session starts =========================
platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1
rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml
plugins: mock-1.13.0, cov-2.11.0
collected 2 items                                                                                                 

src/tests/test_run.py .                        [ 50%]
src/tests/pipelines/loading/test_nodes.py .    [100%]

========================== 2 passed in 1.94s ==========================
# Au tout d√©but, `pytest` ex√©cute le test sur `test_run.py`, qui montre un exemple de test unitaire avec Kedro. Ensuite, il ex√©cute le seul autre fichier de test pr√©sent `test_nodes.py`. Puisqu'il n'y a aucun probl√®me, cela signifie que le code n'a pas g√©n√©r√© d'erreurs et que, en th√©orie, nous avons correctement r√©ussi √† impl√©menter la fonction de test avec Kedro. C'est alors que nous pouvons rajouter des tests et des conditions dans la fonction.
def test_load_csv_from_bucket(project_id, primary_folder):
    df = load_csv_from_bucket(project_id, primary_folder)
    assert type(df) == pd.DataFrame
    assert df.shape[1] == 16
    assert "purchased" in df
# ### Tests sur les pipelines
# 
# En plus de tests unitaires sur les nodes, il est √©galement possible d'effectuer des tests unitaires sur les pipelines. Cela permet, par exemple, de s'assurer du bon d√©roulement du pipeline en fonction de plusieurs situations (donn√©es incompl√®tes ou manquantes, mauvaise configuration de param√®tres). En respectant le m√™me principe que pour les nodes, nous allons cr√©er le fichier `test_pipeline.py`.
from kedro.runner import SequentialRunner

from purchase_predict.pipelines.loading.pipeline import create_pipeline

def test_pipeline(catalog_test):
    runner = SequentialRunner()
    pipeline = create_pipeline()
    pipeline_output = runner.run(pipeline, catalog_test)
# Nous r√©cup√©rons la fonction `create_pipeline` permettant de cr√©er le pipeline que nous souhaitons tester. Dans le test unitaire, nous instancions un `SequentialRunner`, qui ex√©cutera le pipeline de mani√®re s√©quentielle. Ensuite, nous cr√©ons une instance du pipeline et enfin nous ex√©cuter ce dernier. Remarquons la variable `catalog_test` : il s'agit d'un catalogue de donn√©es sp√©cifiquement cr√©e pour le test. Plut√¥t que d'utiliser celui par d√©faut dans le fichier `catalog.yml`, nous allons pouvoir sp√©cifier des donn√©es propres aux tests qui ne va pas perturber le catalogue d√©j√† pr√©sent.
# 
# Le catalogue de donn√©es repr√©sente lui aussi une fixture que nous rajoutons dans `conftest.py`.
import pytest

from kedro.io import DataCatalog, MemoryDataSet

@pytest.fixture(scope="module")
def project_id():
    return "<PROJECT_GCP>"

@pytest.fixture(scope="module")
def primary_folder():
    return "<NOM_DU_BUCKET>/primary/data-test.csv"

@pytest.fixture(scope="module")
def catalog_test(project_id, primary_folder):
    catalog = DataCatalog({
        "params:gcp_project_id": MemoryDataSet(project_id),
        "params:gcs_primary_folder": MemoryDataSet(primary_folder)
    })
    return catalog
# Cette fonction retourne un `DataCatalog` qui sera envoy√© en entr√©e au pipeline.
# 
# <div class="alert alert-block alert-warning">
#     Il faut respecter les noms des variables sp√©cifi√©s dans le pipeline. 
# </div>
# 
# Pour rappel, le pipeline `loading` √©tait d√©fini de la mani√®re suivante.
def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                load_csv_from_bucket,
                ["params:gcp_project_id", "params:gcs_primary_folder"],
                "primary",
            )
        ]
    )
# L√†-aussi, `pytest` remplacera `catalog_test` par la fixture associ√©e et permettra d'initialiser correctement l'environnement de test.
kedro test========================= test session starts =========================
platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1
rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml
plugins: mock-1.13.0, cov-2.11.0
collected 2 items                                                                                                 

src/tests/test_run.py .                          [ 33%]
src/tests/pipelines/loading/test_nodes.py .      [ 66%]
src/tests/pipelines/loading/test_pipeline.py .   [100%]

========================== 3 passed in 2.17s ==========================
# Le pipeline a √©t√© ex√©cut√© sans probl√®me. Nous pouvons l√†-aussi r√©diger des tests pour le pipeline, qui en soit seront quasi-identiques √† ceux du node car ce pipeline ne contient qu'un seul node et ce dernier n'appelle pas d'autres fonctions.
def test_pipeline(catalog_test):
    runner = SequentialRunner()
    pipeline = create_pipeline()
    pipeline_output = runner.run(pipeline, catalog_test)
    df = pipeline_output["primary"]
    assert type(df) == pd.DataFrame
    assert df.shape[1] == 16
    assert "purchased" in df
# ## ‚úîÔ∏è Conclusion
# 
# Peut-√™tre il s'agit de ton premier test unitaire avec Python : dans tous les cas, tu sais maintenant en √©crire, et c'est une tr√®s bonne pratique !
# 
# - Nous avons vu pourquoi les tests logiciels √©taient indispensables.
# - Nous avons r√©dig√© plusieurs tests unitaires pour le pipeline de collecte des donn√©es.
# 
# > ‚û°Ô∏è Il nous reste maintenant √† d√©finir et r√©diger les <b>tests sur le mod√®le de Machine Learning</b>.
