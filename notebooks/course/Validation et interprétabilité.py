#!/usr/bin/env python
# coding: utf-8

# Pour s'assurer qu'une fois en production, le mod√®le adopte un comportement similaire √† celui rencontr√© lors de la phase exp√©rimentale, nous devons utiliser des outils nous permettant de l'auditer. Bien que les scores permettent d'avoir une id√©e sur les performances globales, elles ne sont pas suffisantes.
# 
# <blockquote><p>üôã <b>Ce que nous allons faire</b></p>
# <ul>
#     <li>√âvaluer les performances du mod√®le</li>
#     <li>Interp√©ter localement le mod√®le</li>
# </ul>
# </blockquote>
# 
# <img src="https://media.giphy.com/media/uvoECTG2uCTrG/giphy.gif" />

# ## Validation du mod√®le
# 
# Les premiers objectifs de la validation permettent de s'assurer que le mod√®le calibr√© respecte bien certaines contraintes qui ne sont pas uniquement li√©es aux performances ou au score.

# In[ ]:


import os
import joblib

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns

sns.set()

model = joblib.load(os.path.expanduser("~/data/model.pkl"))
X_train = pd.read_csv(os.path.expanduser("~/data/X_train.csv"))
X_test = pd.read_csv(os.path.expanduser("~/data/X_test.csv"))
y_train = pd.read_csv(os.path.expanduser("~/data/y_train.csv")).values.flatten()
y_test = pd.read_csv(os.path.expanduser("~/data/y_test.csv")).values.flatten()

y_prob = model.predict_proba(X_test)

# ### Densit√©s des classes
# 
# Nous avons vu que la pr√©cision de notre mod√®le √©tait moins bonne que le rappel. En particulier, avec la courbe de calibration, nous avons pu observer que sur des probabilit√©s pr√©dites (classe positive) inf√©rieures √† $40\%$, les proportions d'observations r√©ellement positives n'adoptaient pas un comportement lin√©aire.
# 
# Pour mieux visualiser ce ph√©nom√®ne, il est courant de repr√©senter les densit√©s des deux classes sur un grahique. On affiche deux histogrammes, qui correspondent aux observations pr√©dites positivement et n√©gativement.

# In[ ]:


plt.figure(figsize=(16, 10))

sns.histplot(y_prob[y_test == 0, 1], alpha=0.5)
plt.axvline(np.median(y_prob[y_test == 0, 1]), 0,1, linestyle="--", label="Median Class 0")
plt.axvline(np.mean(y_prob[y_test == 0, 1]), 0,1, linestyle="-", label="Mean Class 0")

sns.histplot(y_prob[y_test == 1, 1], color="darkorange", alpha=0.4)
plt.axvline(np.median(y_prob[y_test == 1, 1]), 0, 1, linestyle="--", color="darkorange", label="Median Class 1")
plt.axvline(np.mean(y_prob[y_test == 1, 1]), 0, 1, linestyle="-", color="darkorange", label="Mean Class 1")

plt.legend()
plt.xlabel("Predicted probabilites")
plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.xlim(-0.05, 1.05)
plt.title("Density Chart", fontsize=16)
plt.show()

# Pour rappel, nous avions d√©j√† calcul√© la courbe de calibration du mod√®le.

# In[ ]:


from sklearn.calibration import calibration_curve

prob_pos = model.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=20)

plt.figure(figsize=(16, 10))
plt.plot([0, 1], [0, 1], "k--", label="Perfectly calibrated", alpha=0.6)
plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Model")
plt.ylabel("Fraction of positives")
plt.xlabel("Predicted probabilites")
plt.legend()
plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.title("Calibration Curve")
plt.show()

# Bien que la courbe de calibration ne soit pas choquante, nous observons sur les densit√©s des classes que les deux distributions sont **asym√©triques** avec une moyenne √† gauche par rapport √† la m√©diane, traduisant d'un √©talement vers la gauche. Bien que ce soit attendu pour la classe positive, cela l'est moins pour la classe n√©gative. En effet, cette derni√®re devrait, pour un mod√®le parfait, √™tre √©tal√©e vers la droite, donc la majorit√© des observations sont √† gauche.
# 
# En soit, ce graphe ne bloque pas la validation du mod√®le, elle traduit simplement de mani√®re visuelle que le mod√®le a plus de difficult√©s √† pr√©dire avec une forte confiance des probabilit√©s basses.

# ### Courbe ROC
# 
# La courbe ROC trace la courbe de la sensibilit√© du mod√®le en fonction de sa sp√©cifit√©. En d'autres termes, il s'agit de tracer l'√©volution du taux de vrais positifs en fonction du taux de faux positifs.

# In[ ]:


from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])

plt.figure(figsize=(16, 10))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:2.1f}%)'.format(auc(fpr, tpr) * 100))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([-0.01, 1.01])
plt.ylim([-0.01, 1.01])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.title("ROC Curve", fontsize=16)
plt.legend(loc="lower right")

# ###¬†Courbe PR
# 
# Une autre courbe √©galement utilis√©e est la courbe PR, qui elle va tracer l'√©volution de la pr√©cision en fonction du rappel.

# In[ ]:


from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve
from sklearn.metrics import PrecisionRecallDisplay

import matplotlib.pyplot as plt

y_pred = model.predict_proba(X_test)

plt.figure(figsize=(16,11))
prec, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1], pos_label=1)
pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=plt.gca())
plt.title("PR Curve", fontsize=16)
plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1, 0))
plt.show()

# La principale diff√©rence entre les courbes ROC et PR, c'est que la pr√©cision et le rappel calculent des taux √† partir des vrais positifs sans se soucier des vrais n√©gatifs. La pr√©cision ne fait pas intervenir 
# 
# √Ä l'inverse de la courbe ROC, la pr√©cision n'utilise pas le TPR, mais la PPV !
# 
# √Ä l'inverse, la courbe ROC utilise toutes les informations.
# 
# Si l'on ne s'int√©resse pas √† la **sp√©cificit√©**, alors la courbe PR peut √™tre int√©ressante √† interpr√©ter. Dans le cas contraire, la courbe ROC pourra fournir plus d'informations.

# ## Interpr√©tation locale
# 
# Au cours des derni√®res ann√©es, les mod√®les de Machine Learning atteignaient des performances de plus en plus √©lev√©es, d√©passant parfois les performances r√©alis√©es par des humains sur certaines t√¢ches pr√©cises. La comp√©tition annuelle ILSVRC, o√π des √©quipes de recherche √©valuent des algorithmes de traitement d'image sur le jeu de donn√©es ImageNet, voyait les meilleurs taux d'erreurs √† $26\%$. 
# 
# En 2012, l'av√®nement des r√©seaux de neurones et de l'apprentissage profond, et plus particuli√®rement les r√©seaux de neurones convolutifs ont permis d'abaisser le taux d'erreur √† $16\%$. Depuis, les r√©seaux de neurones sont majoritairement utilis√©s dans cette comp√©tition et d'autres semblables.
# 
# <img src="https://dv495y1g0kef5.cloudfront.net/training/data_scientist_airbnb/img/interp1.png" />
# 
# En contrepartie, les r√©seaux de neurones sont souvent consid√©r√©s comme des ¬´ bo√Ætes noires ¬ª, c'est-√†-dire des algorithmes dont le fonctionnement est opaque et difficile √† interpr√©ter. En effet, du fait du tr√®s grand nombre de param√®tres (plusieurs dizaines voir centaines de millions), l'interpr√©tation de ces mod√®les n'est pas faisable.
# 
# Les r√©seaux de neurones sont un exemple de ¬´ bo√Ætes noires ¬ª, tout comme le sont les algorithmes d'ensemble learning que nous avons construit tels que Random Forest ou XGBoost.
# 
# Le terme **transparence des algorithmes** est propre au contexte √©tudi√©, et il n'existe pas une d√©finition unique. La transparence peut faire r√©f√©rence √† la connaissance de la d√©cision prise par l'algorithme, au degr√© d'exactitude de la pr√©diction ou √† l'importance des variables sur la pr√©diction.
# 
# <a href="https://christophm.github.io/interpretable-ml-book/" target="_blank">Christoph Molnar</a> reprend la d√©finition de l'interpr√©tabilit√© de Tim Miller :
# 
# <p style="text-align: center;">¬´ L'interpr√©tabilit√© est le degr√© √† quel point un humain peut expliquer de mani√®re coh√©rente les pr√©dictions du mod√®le ¬ª</p>
# 
# Sous cette d√©finition, l'interpr√©tabilit√© est une partie int√©grante de la transparence, qui vise √† √™tre capable d'expliquer de mani√®re pr√©cise et consistante la pr√©diction, que ce soit pour une observation ou dans le comportement global de l'algorithme.
# 
# ### Mod√®les naturellement interpr√©tables
# 
# Que signifie un mod√®le naturellement interpr√©table ? Lorsque nous avons r√©alis√© la r√©gression lin√©aire, nous avons √©t√© capable de calculer directement l'impact de chaque variable sur la pr√©diction. De plus, du fait de l'hypoth√®se de lin√©arit√© entre les variables, il est facile d'expliquer comment, **pour un individu donn√©, le r√©sultat a √©t√© obtenu (i.e. de combien le prix a augment√© ou diminu√©)**. Enfin, le mod√®le suppose initialement **l'ind√©pendance entre les variables**, ce qui permet de consid√©rer les effets crois√©s entre les variables inexistants.
# 
# $$y_i= \beta_0 + \sum_{j=1}^p \beta_j x_{ij} + \varepsilon_i$$
# 
# Autrement dit, chaque variable **est associ√©e d'un "poids" $\beta_j$** : dans le cas o√π toutes les variables sont dans la m√™me unit√© de mesure, cela permet donc de mesure **l'importance de chaque variable**.
# 
# N√©anmoins, chaque individu poss√®de des caract√©ristiques diff√©rentes : et c'est notamment en multipliant la valeur $x_{ij}$ d'une variable d'un individu $x_i$ par le poids $\beta_j$ que l'on peut caract√©riser, **pour cet individu**, l'importance et le r√¥le de la variable sur la pr√©diction.
# 
# En revanche, les mod√®les qui permettent d'atteindre des performances plus √©lev√©es, sont √©galement plus difficilement interpr√©tables. Le mod√®le XGBoost est construit de mani√®re r√©cursive, et chaque arbre d√©pends des pr√©c√©dents. Pour expliquer la pr√©diction d'une observation $x$, il est n√©cessaire de calculer la sortie de chaque arbre, en sachant que les pr√©dicteurs faibles ne cherchent plus √† mod√©liser la variable r√©ponse, mais les pseudo-r√©sidus. C'est la multiplicit√© des arbres (associ√©e √† d'√©ventuels arbres profonds) qui rend la compr√©hension du comportement du mod√®le quasi-impossible.
# 
# Ainsi, au cours des derni√®res ann√©es, la recherche acad√©mique s'est pench√©e sur des m√©thodes d'interpr√©tabilit√© afin de pouvoir expliquer le comportement et les pr√©dictions des algorithmes. Deux types de m√©thodes ont √©t√© d√©velopp√©es.
# 
# ### M√©thode agnostiques
# 
# Les m√©thodes dites **agnostiques** sont ind√©pendantes du mod√®le pr√©dictif utilis√©. Le principal avantage est leur flexibilit√©, puisque ces m√©thodes peuvent √™tre appliqu√©es sans connaissance particuli√®re du mod√®le pr√©dictif, si ce n'est qu'obtenir la pr√©diction $\hat{f}(\mathbf{x})$ pour toute observation $\mathbf{x}$. Ces m√©thodes agnostiques s'intercalent sur des mod√®les bo√Ætes noires. Les PDP (Partial Dependency Plot) furent une des premi√®res m√©thodes d'interpr√©tabilit√©, en estimant les lois marginales des variables sous des hypoth√®ses d'ind√©pendance entre les variables. Plus r√©cemment, d'autres m√©thodes telles que **LIME** ou **Kernel SHAP** ont √©t√© introduites afin de pallier certaines faiblesses des pr√©c√©dentes m√©thodes et de les adapter pour des mod√®les plus complexes et plus co√ªteux en terme de calcul.
# 
# ### M√©thode sp√©cifiques
# 
# Les m√©thodes dites **sp√©cifiques** d√©pendent du mod√®le pr√©dictif utilis√©. Bien que ces m√©thodes soient moins flexibles, elles permettent d'obtenir de meilleurs interpr√©tabilit√© puisqu'elles sont sp√©cifiquement d√©velopp√©es pour un mod√®le pr√©dictif particulier. Ces m√©thodes ne se reposent pas uniquement sur la pr√©diction $\hat{f}(\mathbf{x})$ des observations $\mathbf{x}$, mais utilisent √©galement les propri√©t√©s et m√©thodes de construction d'un mod√®le pour en extraire le plus d'information quant au comportement que celui-ci adopte. Les r√©seaux de neurones sont principalement vis√©s par ces m√©thodes avec **DeepLIFT**, ou les mod√®les √† base d'arbres avec **Tree SHAP**.
# 
# ### Niveaux de granularit√©
# 
# Lorsque le terme d'interpr√©tabilit√© est employ√©, deux niveaux de granularit√© peuvent √™tre distingu√©s en classes de m√©thodes.
# 
# - Les m√©thodes dites **locales**, o√π la m√©thode consiste √† expliquer la pr√©diction d'une observation particuli√®re. Christoph Molnar diff√©rencie l'interpr√©tabilit√© (g√©n√©rale) du mod√®le et appelle l'*explication* le fait de pouvoir pleinement expliquer la pr√©diction pour une observation particuli√®re. DeepLIFT ou Tree SHAP sont des exemples de m√©thodes locales.
# - Les m√©thodes dites **globales**, qui cherchent plut√¥t √† expliquer les tendances du mod√®le sur l'ensemble des pr√©dictions, comme par exemple les lois marginales. PDP ou Tree Interpreter sont des exemples de m√©thodes globales.
# 
# <div class="alert alert-block alert-warning">
#     Ces m√©thodes calculent souvent une approximation pour pouvoir interpr√©ter plus facilement : <b>attention √† la sur-interpr√©tation</b>.
# </div>
#     
# Nous allons nous concentrer ici √† **l'interpr√©tabilit√© locale** du mod√®le.
# 
# ## Valeurs de Shapley
# 
# Les valeurs de Shapley fournissent une m√©thode d'interpr√©tabilit√© **locale** : elles permettent de r√©pondre √† la question ¬´ pourquoi cet utilisateur a une forte probabilit√© d'acheter ? ¬ª. Faisons une petite introduction √† cette m√©thode.
# 
# Les valeurs de Shapley puisent leurs origines dans la th√©orie des jeux coop√©ratifs. Ces valeurs furent calcul√©es par Lloyd Shapley en 1953. Les valeurs de Shapley indiquent la r√©partition √©quitable des gains parmi les joueurs (ou *acteurs*) d'une coalition dans le cadre d'un jeu coop√©ratif. Cette configuration induit une **utilit√© transf√©rable**, puisque l'objectif de cette coalition est de **maximiser** le profit global, pour ensuite redistribuer ce montant parmi les membres de la coalition. Il est important de distinguer la notion d'√©quit√© et d'√©galit√©. Soient trois joueurs $A, B$ et $C$ qui, individuellement, n'apportent aucun gain, mais qui, sous forme de coalition, apportent les gains suivants :
# 
# - la coalition $\{A, B\}$ rapporte $2$ ;
# - la coalition $\{A, C\}$ rapporte $2$ ;
# - la coalition $\{B, C\}$ rapporte $3$ ;
# - la coalition totale $\{A, B, C\}$ rapporte le gain total $4$.
# 
# Dans cet exemple, il est clair que la coalition $\{B, C\}$ est celle qui **contribue** le plus au gain total par rapport aux autres coalitions. Ainsi, pour satisfaire une notion d'√©quit√©, les joueurs de la coalition $\{B, C\}$ doivent recevoir une part plus importante du gain total par rapport au joueur $A$.
# 
# Pour un jeu coop√©ratif √† $p$ joueurs, il peut y avoir $2^p-1$ coalitions non vides possibles, o√π chaque joueur est identifi√© par un indice allant de $1$ √† $p$. Le profit **est suppos√© connu** pour chaque partie de $\{1,‚Ä¶,p\}$, et se quantifie par la **fonction caract√©ristique** $v:\mathcal{P}(\{1,‚Ä¶,p\}) \rightarrow \mathbb{R}$, et v√©rifiant $v(\emptyset)=0$. En pratique, rien ne suppose que les gains d'une coalition soient toujours sup√©rieurs √† la somme des gains de chaque joueur, soit
# 
# $$v \left( \bigcup_i \{i\} \right) \ngeqslant \sum_{i} v(\{i\})$$
# 
# Dans ce cas de figure, un ou plusieurs joueurs auront une valeur de Shapley **inf√©rieure** au gain individuel, car ils contribueront √† faire baisser les gains lors du rassemblement en coalition. Cet √©v√©nement peut survenir dans des cadres classiques de la th√©orie moderne de l'√©conomie (deux entreprises qui coop√®rent ensemble peuvent obtenir un profit moins √©lev√© que si elles ne coop√©raient pas), mais cet aspect est particuli√®rement important en apprentissage supervis√©, ce qui sera d√©taill√© par la suite.
# 
# Shapley a donc d√©termin√© la seule solution qui v√©rifie ces axiomes, √† savoir
# 
# $$\phi_i=\sum_{Z \subseteq\{1, \dots, p\} : j \in Z} \frac{(p-|Z|)!(|Z|-1)!}{p!}\left [ v(Z)-v(Z \backslash \{ j\}) \right ]$$
# 
# o√π $|Z|$ d√©signe la cardinalit√© de l'ensemble $Z$. Cette formule op√®re comme d√©finition des valeurs de Shapley que nous utiliserons dans la mod√©lisation. Notons que le calcul des valeurs de Shapley implique de **conna√Ætre les gains pour toutes les coalitions possibles**. Dans certains domaines (√©conomique par exemple), cela n'est pas toujours possible, notamment lorsque les coalitions ne peuvent pas se reformer (si deux entreprises coop√®rent, leurs gains individuels apr√®s coop√©ration peuvent √™tre diff√©rents des gains individuels avant coop√©ration). Ainsi, $v$ est **enti√®rement d√©termin√©e** et pour tout $C \subseteq \{1, \dots, p\}$, la valeur $v(C)$ est connue.
# 
# ### SHAP
# 
# En 2017, Scott Lundberg propose SHAP comme mesure unifi√©e de l'importance des variables. Son id√©e est la suivante :
# 
# - On consid√®re que les variables sont **les joueurs**.
# - La coalition totale repr√©sente l'ensemble des variables, et le gain correspond √† **la pr√©diction du mod√®le**
# 
# Id√©alement, une valeur de Shapley pour une variable nous indiquerait quelle est sa contribution sur la pr√©diction. Par exemple, une valeur de Shapley proche de $0$ signifierait que la variable n'a pas beaucoup impact√© la pr√©diction, alors qu'une valeur √©lev√©e indiquerait que la variable impacte fortement le prix du logement.
# 
# Avec SHAP, nous allons pouvoir calculer ces valeurs de Shapley (de mani√®re approximative ou exacte pour les arbres de d√©cision).
# 
# Ainsi, Lundberg a montr√© que, pour chaque individu x, les valeurs SHAP sont calcul√©es de sorte √† exprimer la pr√©diction $\hat{f}(\mathbf{x})$ par la somme des contributions des variables :
# 
# $$\hat{f}(\mathbf{x})=\frac{1}{1+\exp \left(-\phi_0-\sum_{j=1}^p \phi_j \right)}$$
# 
# Avec $\phi_0$ la moyenne des valeurs de Shapley pour la classe positive. Les valeurs de Shapley vont √™tre stock√©es dans la variable `shap_values`.

# In[ ]:


import shap

# L'objet Explainer
explainer = shap.TreeExplainer(model)
X_shap = X_test.copy()
# On r√©cup√®re les valeurs de Shapley dans la matrice (pour la proba positive)
shap_values = explainer.shap_values(X_shap)[1]

# Pour interpr√©ter plus facilement les valeurs de Shapley d'une observation, nous allons d√©composer chaque variable sur un diagramme en b√¢tons.

# In[ ]:


# Cette fonction permet d'afficher les valeurs de Shapley sous forme de diagramme en b√¢tons
def plot_shapley_values(index):
    shap_df = pd.DataFrame.from_dict({
        'Variable': X_shap.columns + " (" + X_shap.iloc[0, :].values.astype(str) + ")",
        'Valeur de Shapley': shap_values[index, :]
    })

    # Pour rappel, la pr√©diction est √©gale √† la somme des valeurs de Shapley + la valeur moyenne
    prob = explainer.expected_value[1] + shap_df['Valeur de Shapley'].sum()
    prob = 1 / (1 + np.exp(-prob))

    plt.figure(figsize=(13,10))
    sns.barplot(
        y='Variable',
        x='Valeur de Shapley',
        data=shap_df.sort_values('Valeur de Shapley', ascending=False)
    )
    plt.title(
        "Probabilit√© : {:2.2f}%".format(prob * 100),
        fontsize=18
    )
    plt.yticks(fontsize=13)
    
plot_shapley_values(8)

# Pour ce logement, le mod√®le est ind√©cis puisqu'il pr√©dit presque $50/50$. Ce que l'on remarque, c'est que pour cet utilisateur, ce produit en particulier contribue fortement √† faire baisser la probabilit√©.
# 
# Prenons un autre utilisateur.

# In[ ]:


plot_shapley_values(1)

# En revanche, pour cet utilisateur, il y a une tr√®s forte probabilit√© d'achat. Les variables les plus impactantes sont le nombre de vues et de sessions.
# 
# Dans certains cas, il est possible d'interpr√©ter globalement en affichant les valeurs de Shapley de chaque variable et de chaque observation. La variation de couleur indique si la variable a une grande valeur ou non.

# In[ ]:


shap.summary_plot(shap_values, X_shap, plot_size=0.8)

# Alors que l'on observe une tendance croissante pour le `num_views_session` ou `duration`, cela est plus difficile pour `product_id`, `brand` ou `category`, ce qui est pr√©visible puisque nous avions r√©alis√© un encodage par dictionnaire : il n'y a donc pas de relation d'ordre entre les variables.
# 
# Regardons en d√©tail les valeurs de Shapley uniquement pour la variable `product_id`.

# In[ ]:


shap.dependence_plot("product_id", shap_values, X_shap, interaction_index=None)

# Il est int√©ressant de voir que certains paliers se forment : sp√©cifiquement entre 2e7 et 3e7, il y a certains produits qui influencent positivement la probabilit√© d'acheter, car leur valeurs s'√©l√®vent √† $4$.
# 
# <div class="alert alert-block alert-warning">
#     La valeur de Shapley ne repr√©sente pas une probabilit√© ! Il s'agit du calcul avant le passage par la fonction logistique.
# </div>

# In[ ]:


shap.dependence_plot("hour", shap_values, X_shap, interaction_index=None)

# Pour l'heure de visite, nous observons √©galement un comportement moyen d√©croissant entre 5h et 17h, puis une augmentation jusqu'√† 00h. Cette baisse peut s'expliquer par le fait qu'√† partir de 17h, il y a beaucoup plus de connexions qu'en milieu de nuit, et que ces utilisateurs sont plus souvent ind√©cis que ceux visitant le site la nuit.

# In[ ]:


shap.dependence_plot("num_views_session", shap_values, X_shap, interaction_index=None)

# Contrairement √† ce que nous pourrions penser, les valeurs de Shapley sont √©lev√©es pour les faibles valeurs de `num_views_sessions`. √Ä partir de $5$ visites dans la m√™me session, les valeurs de Shapley sont plus diffuses mais sont en moyenne de l'ordre de $-0.5$, faisant ainsi l√©g√®rement baisser la probabilit√©.
# 
# <div class="alert alert-block alert-warning">
#     Il faut toujours garder en t√™te qu'il y a des interactions entre les variables, et que le fait d'avoir des valeurs de Shapley √©lev√©es pour de faibles valeurs ne peut pas se r√©sum√©rer √† cette seule variable.
# </div>

# ## ‚úîÔ∏è Conclusion
# 
# Cette √©tape de validation est importante, puisque lorsque nous automatiserons l'entra√Ænement du mod√®le, seuls ces graphiques et ces interpr√©tations permettront de v√©rifier que le mod√®le est r√©ellement performant, et pas uniquement en terme de m√©triques.
# 
# - Nous avons valid√© le mod√®le √† l'aide de graphiques.
# - Nous avons interpr√©t√© localement certaines observations avec les valeurs de Shapley.
# 
# > ‚û°Ô∏è Maintenant que avons construit notre pipeline ML, de la transformation des donn√©es √† la validation, il nous faut l'appliquer non pas sur un √©chantillon d'un jour d'historique, mais de $7$ jours d'historique.
