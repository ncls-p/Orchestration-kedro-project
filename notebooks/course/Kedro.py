#!/usr/bin/env python
# coding: utf-8

# Il est temps d'organiser tout notre pipeline ML, qui est actuellement sÃ©parÃ© dans plusieurs Notebooks. Pour nous aider, nous allons utiliser **Kedro**, un outil open-source permettant de crÃ©er des projets de Machine Learning reproductibles, maintenables et modulaires (i.e. plusieurs fichiers), le tout sans trop d'efforts. C'est donc un outil sur-mesure pour les ML Engineers !
# 
# <blockquote><p>ğŸ™‹ <b>Ce que nous allons faire</b></p>
# <ul>
#     <li>CrÃ©er un premier projet Kedro et comprendre son architecture</li>
#     <li>Comprendre les concepts de Kedro</li>
#     <li>Construire un premier pipeline de traitement de donnÃ©es</li>
# </ul>
# </blockquote>
# 
# <img src="https://media.giphy.com/media/ZVUu5Pm23hDZS/giphy.gif" width="300" />

# ## Kedro
# 
# Qu'est-ce que Kedro, et pourquoi avons-nous besoin de cet outil ? La mÃ©thode classique pour construire des modÃ¨les de Machine Learning est d'utiliser Jupyter Notebook. Mais cette mÃ©thode n'est pas du tout viable, notamment lorsqu'il s'agit de dÃ©ployer le modÃ¨le en production dans un futur proche. Face Ã  cette situation, on prÃ©fÃ¨re donc construire un projet entier, dont les Notebooks sont en rÃ©alitÃ© des phases de recherche, d'expÃ©rimentation mais ne constituent pas en soi le coeur de sujet du projet. DÃ¨s lors que l'on met en place une architecture de code source, il est nÃ©cessaire d'adopter de bonnes pratiques, aussi bien hÃ©ritÃ©es des environnements IT que celles utilisÃ©es par les Data Scientists.
# 
# <img src="https://repository-images.githubusercontent.com/182067506/4c724a00-48f4-11ea-84a5-cf8292b07d8e" width="600">
# 
# Kedro a Ã©tÃ© dÃ©veloppÃ© pour appliquer ces bonnes pratiques tout au long d'un projet de Machine Learning.
# 
# - Ã‰viter au maximum de dÃ©pendre de Jupyter Notebooks qui empÃªche la production d'un code source maintenable et reproductible.
# - AmÃ©liorer la collaboration entre les diffÃ©rents acteurs (Data Scientists, Data Engineers, DevOps) aux compÃ©tences diverses dans un projet.
# - Augmenter l'efficacitÃ© en appliquant la modularitÃ© du code, les sÃ©parations entre les donnÃ©es et leur utilisation ou encore en optimisant les exÃ©cutions de traitements atomiques.
# 
# En bref, Kedro nous permet d'avoir un projet Python **entiÃ¨rement pensÃ©** pour le Machine Learning et optimisÃ© dans ce sens. Il existe d'autres alternatives Ã  Kedro (comme Kubeflow qui se base sur Kubernetes), mais il a l'avantage d'Ãªtre rapide Ã  prendre en main et possÃ¨de une communautÃ© dÃ©jÃ  active.
# 
# ### Premiers pas avec Kedro
# 
# Essayons de crÃ©er un premier projet avec Kedro que nous allons nommer `purchase-predict`.
# 
# <div class="alert alert-danger">
#     La version que nous utiliserons est 0.17.0 : en utilisant une version plus rÃ©cente, il se peut que des erreurs de compatibilitÃ© surviennent. Il est donc conseillÃ© d'utiliser la version 0.17.0 en local pour suivre le cours.
# </div>
# 
# CrÃ©ons un nouveau terminal et un nouveau projet Kedro.
kedro new
# Il nous est demandÃ© un nom de projet. Nous laissons ensuite les autres informations vides (la valeur par dÃ©faut est affichÃ©e entre crochets).
Project Name:
=============
Please enter a human readable name for your new project.
Spaces and punctuation are allowed.
 [New Kedro Project]: purchase-predict

Repository Name:
================
Please enter a directory name for your new project repository.
Alphanumeric characters, hyphens and underscores are allowed.
Lowercase is recommended.
 [purchase-predict]: 

Python Package Name:
====================
Please enter a valid Python package name for your project package.
Alphanumeric characters and underscores are allowed.
Lowercase is recommended. Package name must start with a letter or underscore.
 [purchase_predict]: 
# Avec cette commande, Kedro gÃ©nÃ¨re le dossier `purchase-predict` et y configure une architecture par dÃ©faut.
â”œâ”€â”€ conf
â”‚Â Â  â”œâ”€â”€ base
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ catalog.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ credentials.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logging.yml
â”‚Â Â  â”‚Â Â  â””â”€â”€ parameters.yml
â”‚Â Â  â”œâ”€â”€ local
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ 01_raw
â”‚Â Â  â”œâ”€â”€ 02_intermediate
â”‚Â Â  â”œâ”€â”€ 03_primary
â”‚Â Â  â”œâ”€â”€ 04_feature
â”‚Â Â  â”œâ”€â”€ 05_model_input
â”‚Â Â  â”œâ”€â”€ 06_models
â”‚Â Â  â”œâ”€â”€ 07_model_output
â”‚Â Â  â””â”€â”€ 08_reporting
â”œâ”€â”€ docs
â”‚Â Â  â””â”€â”€ source
â”‚Â Â      â”œâ”€â”€ conf.py
â”‚Â Â      â””â”€â”€ index.rst
â”œâ”€â”€ logs
â”‚Â Â  â””â”€â”€ journals
â”œâ”€â”€ notebooks
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ setup.cfg
â””â”€â”€ src
    â”œâ”€â”€ purchase_predict
    â”‚Â Â  â”œâ”€â”€ cli.py
    â”‚Â Â  â”œâ”€â”€ hooks.py
    â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”œâ”€â”€ pipelines
    â”‚Â Â  â”‚Â Â  â””â”€â”€ __init__.py
    â”‚Â Â  â”œâ”€â”€ run.py
    â”‚Â Â  â””â”€â”€ settings.py
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.py
    â””â”€â”€ tests
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ pipelines
        â”‚Â Â  â””â”€â”€ __init__.py
        â””â”€â”€ test_run.py
# DÃ©taillons tout d'abord chaque dossier de premier niveau.
# 
# - `conf` contient tous les fichiers de configuration des paramÃ¨tres (code, modÃ¨le) ainsi que les clÃ©s et secrets nÃ©cessaires.
# - `data` contient plusieurs dossiers qui correspondent aux donnÃ©es utilisÃ©es ou produits Ã  chaque Ã©tape du pipeline (base d'apprentissage, matrices des *features*, modÃ¨le sÃ©rialisÃ©).
# - `docs` contient des fichiers de documentation.
# - `logs` contient les journaux d'Ã©vÃ©nements de Kedro.
# - `notebooks` permet de stocker des notebooks.
# - `src` contient tous les codes nÃ©cessaire pour crÃ©er les pipelines.
# 
# C'est notamment dans le dossier `src` que nous dÃ©velopperons les briques Ã©lÃ©mentaires et que nous les connecterons ensembles afin de former les diffÃ©rents pipelines.
# 
# Avant de rentrer dans les concepts de Kedro, il est **fortement conseillÃ©** (si ce n'est indispensable) de crÃ©er un environnement virtuel Ã  la racine du projet.
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
# ## Concepts de Kedro
# 
# Kedro ne se contente pas uniquement de crÃ©er un projet : il va Ã©galement apporter des fonctionnalitÃ©s trÃ¨s puissantes qui en font sa popularitÃ©.
# 
# ### Data Catalog
# 
# Pour utiliser des donnÃ©es tout au long d'un pipeline ML, il est dÃ©conseillÃ© d'inscrire des chemins d'accÃ¨s en dur dans le code. Il est prÃ©fÃ©rable d'attribuer des noms Ã  des donnÃ©es existantes que l'on utilise ou celles que l'on crÃ©e. C'est rÃ´le du Data Catalog : nous allons dÃ©finir en amont des rÃ©fÃ©rentiels de donnÃ©es avec des noms associÃ©s. Le Data Catalog est rÃ©fÃ©rencÃ© dans le fichier `conf/base/catalog.yml`.
# 
# Par dÃ©faut, Kedro propose plusieurs sous-dossiers dans `data` qui permet de mieux organiser les donnÃ©es.
# 
# - `raw`, `intermediate` et `primary` font rÃ©fÃ©rence aux donnÃ©es brutes, celles ayant subi des traitements intermÃ©diaires et celles prÃªtes Ã  Ãªtre encodÃ©es.
# - `feature` contiendrait la base d'apprentissage $(X,y)$ encodÃ©e.
# - `model_input` contiendrait les Ã©chantillons d'entraÃ®nement et de test fournis au modÃ¨le.
# - `models` contiendrait le ou les modÃ¨les sÃ©rialisÃ©s.
# - `model_output` et `reporting` contiendraient les sorties du modÃ¨les ainsi que les graphes pour valider et interpÃ©ter.
# 
# <div class="alert alert-block alert-info">
#     â„¹ï¸ Bien entendu, nous ne sommes pas tenu de suivre exactement cette structure, il s'agit plutÃ´t d'une organisation par dÃ©faut proposÃ©e par Kedro.
# </div>
# 
# Dans notre cas, nous n'allons pas effectuer la transformation des donnÃ©es avec Kedro, puisque ce sera une tÃ¢che Spark SQL qui en sera chargÃ©e. Ainsi, nous aurons uniquement le donnÃ©es transformÃ© qui subira ensuite l'encodage. Nous considÃ©rons donc que l'Ã©chantillon que recevra Kedro sera situÃ© dans `primary` et sera nommÃ© `primary.csv`.
# 
# Ã€ partir de ces donnÃ©es `primary.csv`, nous encoderons vers un nouveau fichier `dataset.csv` dans le dossier `feature`, dont nous rÃ©cupÃ©rerons les sous-ensembles d'apprentissage et de test. Pour rÃ©fÃ©rencer tous ces fichiers dans le Data Catalog, nous Ã©ditons le fichier `conf/base/catalog.yml` en spÃ©cifiant le nom, le type de donnÃ©es et le chemin en relatif par rapport au dossier racine.
# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html

primary:
  type: pandas.CSVDataSet
  filepath: data/03_primary/primary.csv

dataset:
  type: pandas.CSVDataSet
  filepath: data/04_feature/dataset.csv

X_train:
  type: pandas.CSVDataSet
  filepath: data/05_model_input/X_train.csv

y_train:
  type: pandas.CSVDataSet
  filepath: data/05_model_input/y_train.csv

X_test:
  type: pandas.CSVDataSet
  filepath: data/05_model_input/X_test.csv

y_test:
  type: pandas.CSVDataSet
  filepath: data/05_model_input/y_test.csv
# L'avantage du Data Catalog est la flexibilitÃ© d'utilisation. PlutÃ´t que de spÃ©cifier le chemin d'accÃ¨s dans chaque fichier Python, nous pouvons simplement inscrire `primary` comme argument, et Kedro va automatiquement charger en mÃ©moire (ici au format CSV avec `pandas`) ce jeu de donnÃ©es. Ainsi, nous pouvons Ã  tout moment modifier la valeur du chemin `filepath` ici sans altÃ©rer tous les fichiers Python.
# 
# ### Nodes et Pipelines
# 
# Parmi les concepts les plus importants, nous retrouvons celui des **nodes** et des **pipelines**.
# 
# Un **node** est un Ã©lÃ©ment unitaire qui reprÃ©sente une tÃ¢che. Par exemple, nous pouvons imaginer un node pour encoder le jeu de donnÃ©es, un autre pour construire les sous-ensembles d'entraÃ®nement et de test, et un dernier pour calibrer un modÃ¨le de Machine Learning.
# 
# Un **pipeline**, Ã  l'instar des pipelines de donnÃ©es, est une succession de nodes qui peuvent Ãªtre assemblÃ©s en sÃ©quence ou en parallÃ¨le.
# 
# Les pipelines sont une partie trÃ¨s importante dans Kedro. Reprenons le pipeline d'expÃ©rimentation oÃ¹ l'on entraÃ®ne un modÃ¨le de Machine Learning.
# 
# <img src="https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/kedro2.png" />
# 
# Ce qui est important Ã  voir, c'est le caractÃ¨re sÃ©quencÃ© entre les tÃ¢ches. En particulier, **impossible d'entraÃ®ner un modÃ¨le sans avoir encodÃ© les donnÃ©es**, tout comme il est impossible d'Ã©valuer un modÃ¨le si l'on n'en a pas.
# 
# Kedro va nous permettre de crÃ©er ces pipelines, garantissant que les **artifacts** (donnÃ©es construites, modÃ¨les, etc) vont Ãªtre disponibles pour les autres nodes du pipeline. C'est un outil essentiel car il va nous assurer que les traitements sont homogÃ¨nes et que lorsque l'on souhaitera entraÃ®ner un nouveau modÃ¨le par exemple, les donnÃ©es subiront exactement le mÃªme traitement puisqu'elles passeront par le mÃªme pipeline. On Ã©vite ainsi les risques d'oubli ou d'erreur de cohÃ©rence entre deux exÃ©cutions successives, chose qui arrive plus souvent que l'on ne l'imagine avec les Jupyter Notebooks.

# ## Premier pipeline
# 
# Nous allons crÃ©er ensemble un premier pipeline qui va contenir deux noeuds.
# 
# - Un premier qui va se charger d'encoder le jeu de donnÃ©es `primary`.
# - Un autre qui va sÃ©parer la base de donnÃ©es en deux sous-ensembles d'entraÃ®nement et d'apprentissage.
# 
# CommenÃ§ons tout d'abord par tÃ©lÃ©charger le fichier d'Ã©chantillon dans le dossier `data/03_primary`.
cp ~/data/primary.csv ~/purchase-predict/data/03_primary/primary.csv
# CrÃ©ons un dossier `processing` dans `src/purchase_predict/pipelines`. Nous allons y ajouter deux fichiers Python `nodes.py` et `pipeline.py`.
# 
# - Le fichier `nodes.py` contient les dÃ©finitions des fonctions qui seront utilisÃ©es par les nodes.
# - Le fichier `pipeline.py` permet de construire le pipeline Ã  partir de nodes qui utiliseront les fonctions du fichier `nodes.py`.
# 
# Puisque nous avons deux noeuds, nous devons construire deux fonctions.
# 
# ### Noeud `encode_features`
# 
# La fonction `encode_features` va rÃ©cupÃ©rer `dataset`, qui correspond au fichier CSV ayant subit les transformations (notamment via Spark SQL).
import pandas as pd

from typing import Dict, Any

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

def encode_features(dataset: pd.DataFrame) -> pd.DataFrame:
    """
    Encode features of data file.
    """
    features = dataset.drop(["user_id", "user_session"], axis=1).copy()

    encoders = []
    for label in ["category", "sub_category", "brand"]:
        features[label] = features[label].astype(str)
        features.loc[features[label] == "nan", label] = "unknown"
        encoder = LabelEncoder()
        features.loc[:, label] = encoder.fit_transform(features.loc[:, label].copy())
        encoders.append((label, encoder))

    features["weekday"] = features["weekday"].astype(int)
    return dict(features=features, transform_pipeline=encoders)
# Cette fonction retourne le DataFrame `features`, qui correspond aux donnÃ©es encodÃ©es.
# 
# ### Noeud `split_dataset`
# 
# L'autre fonction, `split_dataset`, opÃ¨re simplement une sÃ©paration en deux sous-Ã©chantillons. L'argument `test_ratio` permettra de spÃ©cifier la proportion d'observation Ã  considÃ©rer dans le sous-Ã©chantillon de test.
def split_dataset(dataset: pd.DataFrame, test_ratio: float) -> Dict[str, Any]:
    """
    Splits dataset into a training set and a test set.
    """
    X = dataset.drop("purchased", axis=1)
    y = dataset["purchased"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_ratio, random_state=40
    )

    return dict(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)
# La fonction retourne les quatre DataFrames.
# 
# ### Construction du pipeline
# 
# Pour entamer la construction du pipeline, nous allons tout d'abord dÃ©finir un **paramÃ¨tre** Kedro, le `test_ratio`. En effet, il s'agit d'un paramÃ¨tre de configuration qui doit Ãªtre initialisÃ© au prÃ©alable, et plutÃ´t que d'inscrire en dur dans le code la valeur du ratio pour l'ensemble de test, tout comme le Data Catalog, le fichier `parameters.yml` dans le dossier `conf/base` permet de centraliser tous les paramÃ¨tres du modÃ¨le, Cloud, de traitement de donnÃ©es, etc.
test_ratio: 0.3
# Ã€ partir de lÃ , nous pouvons construire notre pipeline. Pour cela, nous utilisons l'objet `Pipeline` de Kedro, qui s'attends Ã  une liste de `node`. Chaque instance de `node` attends trois paramÃ¨tres.
# 
# - Le nom de la fonction Python qui sera appelÃ©e.
# - Les arguments de la fonction (sous forme de liste ou dictionnaire).
# - Les sorties du modÃ¨les (sous forme de liste ou dictionnaire).
from kedro.pipeline import Pipeline, node

from .nodes import encode_features, split_dataset

def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                encode_features,
                "primary",
                dict(features="dataset", transform_pipeline="transform_pipeline")
            ),
            node(
                split_dataset,
                ["dataset", "params:test_ratio"],
                dict(
                    X_train="X_train",
                    y_train="y_train",
                    X_test="X_test",
                    y_test="y_test"
                )
            )
        ]
    )
# Le premier noeud appelle la fonction `encode_features` avec pour argument le jeu de donnÃ©es `primary`, et le rÃ©sultat (un seul) retournÃ© par la fonction sera stockÃ© dans le jeu de donnÃ©es `dataset`.
# 
# Le deuxiÃ¨me noeud nÃ©cessite le jeu de donnÃ©es `dataset` ainsi que le paramÃ¨tre `test_ratio`, et retourne les 4 DataFrames qui correspond aux sous-ensembles.
# 
# Notre pipeline est donc en place, et nous pouvons la visualiser ci-dessous.
# 
# <img src="https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/kedro1.jpg" width="800" />
# 
# Avant de lancer le pipeline, installons les quelques dÃ©pendances nÃ©cessaires dans l'environnement virtuel (qui est vierge par dÃ©faut).
pip install pandas scikit-learn
# Pour terminer, il faut crÃ©er une instance du pipeline pour pouvoir l'exÃ©cuter. Toutes les instances sont dÃ©finies dans le fichier `hooks.py` Ã  la racine de `src/purchase_predict`. Ajoutons l'importation suivante.
from purchase_predict.pipelines.processing import pipeline as processing_pipeline
# Nous importons donc le fichier `pipeline` dans `pipelines.processing` dont nous crÃ©ons l'alias `processing_pipeline`. En appelant la fonction `processing_pipeline.create_pipeline()`, cela va instancier un nouveau pipeline qui contient les deux noeuds.
# 
# Ces instanciations doivent Ãªtre dÃ©finies dans la fonction `register_pipelines`, qui retourne un dictionnaire oÃ¹ chaque clÃ© est le nom du pipeline et la valeur l'objet pipeline associÃ©.
@hook_impl
def register_pipelines(self) -> Dict[str, Pipeline]:
    """Register the project's pipeline.

    Returns:
        A mapping from a pipeline name to a ``Pipeline`` object.

    """
    p_processing = processing_pipeline.create_pipeline()

    return {"processing": p_processing}
# C'est notamment ici qu'il sera Ã©galement possible d'imbriquer sÃ©quentiellement plusieurs pipelines entre-eux.
# 
# Notre pipeline est donc prÃªt, nous pouvons l'exÃ©cuter.
kedro run --pipeline processing2021-01-04 17:45:19,425 - kedro.framework.session.session - WARNING - Unable to git describe /home/jovyan/purchase_predict
2021-01-04 17:45:19,463 - root - INFO - ** Kedro project purchase_predict
2021-01-04 17:45:19,486 - kedro.io.data_catalog - INFO - Loading data from `primary` (CSVDataSet)...
2021-01-04 17:45:19,565 - kedro.pipeline.node - INFO - Running node: encode_features([primary]) -> [dataset]
2021-01-04 17:45:19,597 - kedro.io.data_catalog - INFO - Saving data to `dataset` (CSVDataSet)...
2021-01-04 17:45:19,760 - kedro.runner.sequential_runner - INFO - Completed 1 out of 2 tasks
2021-01-04 17:45:19,760 - kedro.io.data_catalog - INFO - Loading data from `dataset` (CSVDataSet)...
2021-01-04 17:45:19,817 - kedro.io.data_catalog - INFO - Loading data from `params:test_ratio` (MemoryDataSet)...
2021-01-04 17:45:19,817 - kedro.pipeline.node - INFO - Running node: split_dataset([dataset,params:test_ratio]) -> [X_test,X_train,y_test,y_train]
2021-01-04 17:45:19,828 - kedro.io.data_catalog - INFO - Saving data to `X_train` (CSVDataSet)...
2021-01-04 17:45:19,918 - kedro.io.data_catalog - INFO - Saving data to `y_train` (CSVDataSet)...
2021-01-04 17:45:19,950 - kedro.io.data_catalog - INFO - Saving data to `X_test` (CSVDataSet)...
2021-01-04 17:45:19,986 - kedro.io.data_catalog - INFO - Saving data to `y_test` (CSVDataSet)...
2021-01-04 17:45:19,999 - kedro.runner.sequential_runner - INFO - Completed 2 out of 2 tasks
2021-01-04 17:45:19,999 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.
2021-01-04 17:45:19,999 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step
# En dÃ©roulant le dossier `data/05_model_input`, nous devrions voir apparaÃ®tre les 4 fichiers CSV gÃ©nÃ©rÃ©s par le pipeline.
# 
# ### Visualisation des pipelines
# 
# La derniÃ¨re dÃ©pendance `kedro-viz` peut Ãªtre utile car elle permet de visualiser les diffÃ©rents pipelines directement depuis le navigateur.
pip install kedro-viz
# Pour lancer le serveur de visualisation, il suffit d'exÃ©cuter la commande suivante (attention de vÃ©rifier que l'environnement virtuel est bien activÃ©).
kedro viz --port 4141
# Une fois lancÃ©, nous pouvons y accÃ©der <a href="jupyter://user-redirect/proxy/4141/" target="_blank">par ce lien</a>.

# ## âœ”ï¸ Conclusion
# 
# Tu viens de mettre en place ton premier pipeline avec Kedro !
# 
# - Nous avons vu crÃ©Ã© notre premier projet avec Kedro.
# - Nous avons dÃ©taillÃ© les concepts importants que l'on rencontre avec Kedro.
# - Nous avons mis en place un premier pipeline.
# 
# > â¡ï¸ AprÃ¨s avoir construit le pipeline de traitement de donnÃ©es, place au **pipeline d'entraÃ®nement du modÃ¨le**.
