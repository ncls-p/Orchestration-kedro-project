#!/usr/bin/env python
# coding: utf-8

# Pr√©c√©demment, nous avons r√©alis√© des tests unitaires sur notre projet Kedro. Les tests unitaires sont rapides √† mettre en place et permettent de v√©rifier ind√©pendamment chaque portion de code.
# 
# Mais lorsqu'il s'agit de mod√®les de Machine Learning, √† quoi correspondent exactement les tests ? Il n'est pas possible de r√©diger des tests unitaires de la m√™me fa√ßon, car un mod√®le fournit justement une pr√©diction, qui n'est a priori pas connu √† l'avance. C'est pourquoi les Data Scientists et ML Engineers ont imagin√© des m√©thodes de test √† r√©aliser sur les mod√®les.
# 
# <blockquote><p>üôã <b>Ce que nous allons faire</b></p>
# <ul>
#     <li>Formaliser les diff√©rents tests qui peuvent survenir</li>
#     <li>R√©diger le tests dans le projet Kedro</li>
# </ul>
# </blockquote>
# 
# <img src="https://media.giphy.com/media/lQ0VQmLuLH7lS/giphy.gif" />

# ## Test de mod√®les
# 
# Lorsque l'on fait r√©f√©rence aux tests de mod√®les, on cherche √† v√©rifier le bon fonctionnement et comportement du mod√®le. Pour cela, nous distinguons deux classes de tests de mod√®les.
# 
# - Les **tests pr√©-entra√Ænement**, qui nous permettent d'identifier des erreurs ou incoh√©rences avant m√™me l'entra√Ænement du mod√®le.
# - Les **tests post-entra√Ænement**, qui vont utiliser le mod√®le entra√Æn√© et inspecter le comportement de ce dernier par rapport √† des sc√©narios de r√©f√©rence que l'on d√©cide en amont.
# 
# <div class="alert alert-block alert-warning">
#     Il n'y a pas de <i>meilleur test possible</i>. Chaque test doit √™tre d√©fini en fonction du contexte, du cas d'application et surtout de l'importance que l'on accorde aux d√©cisions du mod√®le.
# </div>
# 
# ## Les tests pr√©-entra√Ænement
# 
# Les tests pr√©-entra√Ænement permettent d'√©viter de se lancer dans l'entra√Ænement d'un mod√®le si certains crit√®res ne sont pas respect√©s. Tr√®s souvent, ces crit√®res portent sur les donn√©es et les tests, bien que rapide √† mettre en place, permettent d√©j√† de soulever certains points. Parmi les tests qui peuvent √™tre r√©alis√©s avant l'entra√Ænement, nous retrouvons surtout de tests de coh√©rence des donn√©es.
# 
# - Taille du jeu de donn√©es.
# - Format de la variable r√©ponse.
# - Proportion des classes dans la classification binaire.
# - Repr√©sentativit√© de l'√©chantillon par rapport √† la population d'√©tude.
# 
# En soit, il s'agit de tests qui peuvent √™tre r√©dig√©s au m√™me titre que les tests unitaires pr√©c√©dents. D√©finissons les tests du pipeline `processing`. Ce pipeline est compos√© de deux nodes, chacun appelant une fonction.
# 
# - `encode_features`, qui va encoder num√©riquement les variables de `primary`.
# - `split_dataset`, qui va s√©parer le jeu de donn√©es en une base d'apprentissage et une base de test.
# 
# Commen√ßons par la fonction `encode_features` : elle s'attend √† recevoir un DataFrame nomm√© `dataset`. Il y a plusieurs choses que nous devons v√©rifier √† l'issue de cette fonction.
# 
# - La colonne `purchased` est-elle toujours *intacte* dans le sens o√π elle n'est constitu√©e que de $0$ et de $1$ ?
# - Toutes les colonnes sont-elles num√©riques ?
# - Avons-nous suffisamment d'observations pour entra√Æner le mod√®le ?
# - Les proportions de classes positives et n√©gatives sont-elles au moins sup√©rieures √† un seuil ?
# 
# Dans le dossier de test, cr√©ons le dossier `processing` avec le fichier `conftest.py`. Nous allons d√©finir un catalogue de donn√©es pour ce test.
import pytest

from purchase_predict.pipelines.loading.nodes import load_csv_from_bucket

@pytest.fixture(scope="module")
def project_id():
    return "PROJET_GCP"  # TODO : Penser √† changer le nom du projet GCP

@pytest.fixture(scope="module")
def primary_folder():
    return "FICHIER_CSV"  # TODO : Penser √† changer l'URL gs:// du fichier CSV
    
@pytest.fixture(scope="module")
def dataset_not_encoded(project_id, primary_folder):
    return load_csv_from_bucket(project_id, primary_folder)
# Nous r√©-utilisons ici les deux fixtures `project_id` et `primary_folder` d√©j√† pr√©sentes dans les tests du pipeline `loading`. Nous utilisons la fonction `load_csv_from_bucket` pour r√©cup√©rer le jeu de donn√©es de test depuis Cloud Storage afin d'utiliser une version non alt√©r√©e qui serait enregistr√©e en local. En pratique, la fonction `load_csv_from_bucket` aura d√©j√† √©t√© test√©e au pr√©alable par `pytest`, nous pouvons donc l'utiliser ici pour charger les donn√©es.
# 
# <div class="alert alert-block alert-warning">
#     Il faut √©viter de cr√©er des d√©pendances entre les tests : c'est pour cela que l'on red√©fini ici les fixtures sans les importer depuis <code>loading/conftest.py</code>.
# </div>
# 
# Cr√©ons ensuite comme nous l'avions fait le fichier `test_nodes.py` dans le dossier `processing`.
import pandas as pd

from purchase_predict.pipelines.processing.nodes import encode_features

def test_encode_features(dataset_not_encoded):
    df = encode_features(dataset_not_encoded)["features"]
    print(df.head())
# La fonction `test_encode_features` va tester `encode_features` √† partir de la fixture `dataset_not_encoded` que nous venons de d√©finir dans `conftest.py`. Ex√©cutons les tests avec Kedro.
kedro test src/tests/pipelines/processing/ -s
# Avec l'argument `src/tests/pipelines/processing/`, on pr√©cise √† Kedro d'ex√©cuter les tests de mani√®re r√©cursive uniquement dans ce dossier.
# 
# Si tout s'est bien pass√©, alors nos fixtures sont correctement en place et nous pouvons int√©grer les **tests pr√©-entra√Ænement**.
BALANCE_THRESHOLD = 0.1
MIN_SAMPLES = 5000

def test_encode_features(dataset_not_encoded):
    df = encode_features(dataset_not_encoded)["features"]
    # Checking column 'purchased' that all values are either 0 or 1
    assert df['purchased'].isin([0, 1]).all()
    # Checking that all columns are numeric
    for col in df.columns:
        assert pd.api.types.is_numeric_dtype(df.dtypes[col])
    # Checking that we have enough samples
    assert df.shape[0] > MIN_SAMPLES
    # Checking that classes have at least BALANCE_THRESHOLD percent of data
    assert (df['purchased'].value_counts() / df.shape[0] > BALANCE_THRESHOLD).all()
    print(df.head())
# Les constantes `BALANCE_THRESHOLD` et `MIN_SAMPLES` vont bien s√ªr d√©pendrent du contexte. Dans certains situations, `BALANCE_THRESHOLD` devra √™tre amen√© √† $1\%$ ou `MIN_SAMPLES` devra √™tre bien plus cons√©quent.
# 
# √âcrivons maintenant les tests pour la fonction `split_dataset`. Il nous faut rajouter deux fixtures dans le fichier `conftest.py` : une pour le jeu de donn√©es encod√© (apr√®s application de `encode_features`) et une pour le ratio de test.
from purchase_predict.pipelines.processing.nodes import encode_features

@pytest.fixture(scope="module")
def test_ratio():
    return 0.3

@pytest.fixture(scope="module")
def dataset_encoded(dataset_not_encoded):
    return encode_features(dataset_not_encoded)["features"]
# De la m√™me mani√®re, nous ajoutons le test `test_split_dataset` au fichier `test_nodes.py`.
import numpy as np

def test_split_dataset(dataset_encoded, test_ratio):
    X_train, y_train, X_test, y_test = split_dataset(dataset_encoded, test_ratio).values()
    # Checks both sets size
    assert X_train.shape[0] == y_train.shape[0]
    assert X_test.shape[0] == y_test.shape[0]
    assert X_train.shape[0] + X_test.shape[0] == dataset_encoded.shape[0]
    # Note that train_test_split of scikit-learn use np.ceil for test split
    # https://github.com/scikit-learn/scikit-learn/blob/42aff4e2edd8e8887478f6ff1628f27de97be6a3/sklearn/model_selection/_split.py#L1797
    assert np.ceil(dataset_encoded.shape[0] * test_ratio) == X_test.shape[0]
# Lan√ßons les tests.
kedro test src/tests/pipelines/processing/ -s========================= test session starts =========================
platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1
rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml
plugins: mock-1.13.0, cov-2.11.0
collected 2 items

src/tests/pipelines/processing/test_nodes.py ..

========================== 2 passed in 0.65s ==========================
# <div class="alert alert-block alert-info">
#     Les deux points apr√®s le chemin d'acc√®s au fichier signifie que deux fonctions de tests ont √©t√© correctement ex√©cut√©es. En cas d'erreur nous aurions eu un <code>E</code> √† la place et en cas de <i>skipping</i>, nous aurions eu <code>S</code>.
# </div>
# 
# Il ne reste plus qu'√† tester le pipeline `processing` en entier, qui fait appel aux deux fonctions. √Ä noter que dans ce cas, il n'y a pas besoin de tester le jeu de donn√©es interm√©diaire car les tests unitaires sont suppos√©s d√©j√† valides. Pour rappel, le pipeline `processing` est constitu√© de deux nodes.
def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                encode_features,
                "primary",
                "dataset",
            ),
            node(
                split_dataset,
                ["dataset", "params:test_ratio"],
                dict(
                    X_train="X_train",
                    y_train="y_train",
                    X_test="X_test",
                    y_test="y_test",
                ),
            ),
        ]
    )
# Cr√©ons un catalogue de test pour `processing` dans le fichier `conftest.py`.
from kedro.io import DataCatalog, MemoryDataSet

@pytest.fixture(scope="module")
def catalog_test(dataset_not_encoded, test_ratio):
    catalog = DataCatalog({
        "primary": MemoryDataSet(dataset_not_encoded),
        "params:test_ratio": MemoryDataSet(test_ratio)
    })
    return catalog
# Comme pour le pipeline pr√©c√©dent, nous nous basons sur les fixtures `dataset_not_encoded` et `test_ratio` pour cr√©er le catalogue de donn√©es.
# 
# > ‚ùì Pourquoi n'avons-nous pas cr√©e un dataset <code>dataset</code> ?
# 
# Le catalogue de donn√©es de test est identique √† celui utilis√© hors environnement de test, √† la diff√©rence que l'on sp√©cifie nous-m√™me les diff√©rentes entr√©es. Ainsi, remarquons que le premier node va produire en sortie `dataset`. En ex√©cutant le pipeline avec le catalogue de test, `dataset` sera alors stock√© dans le catalogue en tant que `MemoryDataSet` : il sera donc utilisable par le prochain node du pipeline.
# 
# Le contenu de `test_pipeline.py` s'√©crit directement.
from kedro.runner import SequentialRunner

from purchase_predict.pipelines.processing.pipeline import create_pipeline

def test_pipeline(catalog_test):
    runner = SequentialRunner()
    pipeline = create_pipeline()
    pipeline_output = runner.run(pipeline, catalog_test)
    assert pipeline_output["X_train"].shape[0] == pipeline_output["y_train"].shape[0]
    assert pipeline_output["X_test"].shape[0] == pipeline_output["y_test"].shape[0]
# Ex√©cutons l'int√©gralit√© des tests.
kedro test========================= test session starts =========================
platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1
rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml
plugins: mock-1.13.0, cov-2.11.0
collected 6 items

src/tests/test_run.py .                               [ 16%]
src/tests/pipelines/loading/test_nodes.py .           [ 33%]
src/tests/pipelines/loading/test_pipeline.py .        [ 50%]
src/tests/pipelines/processing/test_nodes.py ..       [ 83%]
src/tests/pipelines/processing/test_pipeline.py .     [100%]

========================== 6 passed in 3.34s ==========================
# Et voil√† ! Jusqu'ici, nos deux pipelines `loading` et `processing` sont test√©s. Nous avons r√©alis√© les tests pr√©-entra√Ænement. Mais qu'en est-il de ceux concernant le mod√®le ?

# ## Tests post-entra√Ænement
# 
# Les **tests post-entra√Ænement** vont √™tre ex√©cut√©s une fois le mod√®le calibr√©. Contrairement aux tests pr√©c√©dents, ils sont plus subtils car l'objectif est de mettre en √©vidence certains aspects et comportements particuliers du mod√®le pour v√©rifier qu'ils n'aboutissent pas √† des erreurs ou √† des incoh√©rences qui pourraient avoir d'importantes r√©percussions.
# 
# Pour cela, nous pouvons faire intervenir plusieurs outils.
# 
# - Des exemples de donn√©es dont on conna√Æt (en dehors de la base d'apprentissage) les r√©ponses.
# - Les m√©thodes d'interpr√©tabilit√© avec SHAP par exemple.
# - Des m√©thodes d'√©valuation de propri√©t√©s, comme la r√©gularit√© de la loi jointe.
# 
# Commen√ßons par r√©cup√©rer le mod√®le de boosting que nous avions entra√Æn√© avec la base de test.

# In[ ]:


import os
import joblib
import pandas as pd
import shap

model = joblib.load(os.path.expanduser("~/data/model.pkl"))
X_test = pd.read_csv(os.path.expanduser("~/data/X_test.csv"))
y_test = pd.read_csv(os.path.expanduser("~/data/y_test.csv")).values.flatten()

# On calcul ici les valeurs de Shapley
explainer = shap.TreeExplainer(model)
X_shap = X_test.copy()
shap_values = explainer.shap_values(X_shap)[1]

# ### Tests d'invariance
# 
# Les tests d'invariance nous permettent de d√©finir un ensemble de perturbations √† appliquer sur une ou plusieurs observations pour observer √† quel point cela affecte la sortie du mod√®le.
# 
# Par exemple, supposons qu'un utilisateur, ayant visit√© un produit dont le prix est de 59‚Ç¨. Un test d'invariance consisterai √† dire qu'une variation de $\pm 1‚Ç¨$ ne devrait pas faire varier la probabilit√© d'acheter de $\pm x\%$. Une variation importante pourrait signifier que cette variable a beaucoup d'impact, alors qu'en r√©alit√©, il est peu probable que pour un article de 59‚Ç¨ une variation de 1‚Ç¨ fasse drastiquement augmenter ou baisser la probabilit√©.
# 
# S√©lectionnons une seule observation al√©atoire `x_unit`.

# In[ ]:


x_unit = X_test.loc[4720, :].copy()

print("Pos : {:2.3f}%".format(model.predict_proba([x_unit])[0, 1] * 100))

# La probabilit√© associ√©e ici est d'environ 92%. Appliquons une perturbation de +1‚Ç¨ et -1‚Ç¨.

# In[ ]:


x_unit = X_test.loc[4720, :].copy()
x_unit['price'] += 1
print("Pos : {:2.3f}%".format(model.predict_proba([x_unit])[0, 1] * 100))

x_unit = X_test.loc[4720, :].copy()
x_unit['price'] -= 1
print("Pos : {:2.3f}%".format(model.predict_proba([x_unit])[0, 1] * 100))

# Sur cette observation en particulier, la diff√©rence de probabilit√© est de $0.165\%$. L'impact est **tr√®s limit√©**, indiquant que le mod√®le est r√©gulier au voisinage de ce point.
# 
# Calculons maintenant cette diff√©rence (en valeur absolue) de probabilit√© (pour la classe positive) pour chaque observation.

# In[ ]:


import numpy as np

# On ne s√©lectionne que les articles dont le prix est > √† 1‚Ç¨, sinon on aurait un prix ... n√©gatif !
X_test_price = X_test[X_test['price'] > 1]
X_test_price_plus = X_test_price.copy()
X_test_price_plus['price'] += 1
X_test_price_minus = X_test_price.copy()
X_test_price_minus['price'] -= 1

y_price = pd.DataFrame()
y_price["y"] = model.predict_proba(X_test_price)[:, 1]
y_price["y+"] = model.predict_proba(X_test_price_plus)[:, 1]
y_price["y-"] = model.predict_proba(X_test_price_minus)[:, 1]
y_price["abs_delta"] = np.abs(y_price["y-"] - y_price["y+"])
y_price.sort_values("abs_delta", ascending=False).head(n=10)

# Nous pouvons voir que pour une dizaine d'observations au moins, cette variable de 1‚Ç¨ contribue √† un diff√©rentiel de 20% √† 30% sur la probabilit√© pr√©dite (ce qui est tout de m√™me √©lev√©).
# 
# Regardons de quelles observations il s'agit.

# In[ ]:


idxs = list(y_price.sort_values("abs_delta", ascending=False).head(n=100).index)
X_test_price.loc[idxs, :]

# Nous aurions pu penser qu'il s'agit d'articles dont le prix est faible. Et pourtant, l'ordre de grandeur est de 20 √† 50‚Ç¨ pour la plupart des articles. Regardons les valeurs de Shapley de ces m√™mes observations.

# In[ ]:


shap.summary_plot(shap_values[idxs, :], X_shap.loc[idxs, :], plot_size=0.8)

# Au global, l'impact est limit√©.

# In[ ]:


print("√âcart-type : {:2.2f}%".format(y_price["abs_delta"].std()))
print("Proportion : {:2.2f}%".format(
    y_price[y_price["abs_delta"] < 0.05].shape[0] / y_price.shape[0] * 100
))

# En tra√ßant ce delta pour chaque observation dans l'ordre d√©croissant, nous pouvons voir appara√Ætre un ¬´ coude ¬ª √† partir duquel ce delta stagne.

# In[ ]:


import matplotlib.pyplot as plt

n_obs = 1000

plt.figure(figsize=(16,10))
plt.plot(
    range(n_obs),
    y_price.sort_values("abs_delta", ascending=False).iloc[:n_obs, -1],
    lw=2
)

# Dans notre situation, une variation de 10% **para√Æt raisonnable**. Il serait donc int√©ressant de se concentrer sur les quelques observations qui pr√©sentent une variation de plus de 20% par rapport √† la variable prix.

# ### Test directionnels
# 
# Les tests directionnels semblent proches des tests d'invariance, √† la diff√©rence pr√®s que l'ensemble des perturbations que nous allons appliquer aux observations devraient avoir un effet **connu √† l'avance** sur la sortie du mod√®le.
# 
# 

# In[ ]:


x_unit = X_test.loc[375, :]

model.predict_proba([x_unit])

# L'interpr√©tation locale est d'une grande aide : pourquoi y a-t-il une forte probabilit√© que cet utilisateur ne finalise pas l'achat ?

# In[ ]:


shap.force_plot(explainer.expected_value[0], shap_values[375, :], x_unit, matplotlib=True)

# C'est principalement la dur√©e, qui n'est que de 10 secondes, qui explique pourquoi cet utilisateur ne finaliserai pas l'achat.
# 
# Le but du test directionnel est de se poser la question suivante : et si la dur√©e avait dur√©e 60 secondes de plus, que se passerait-il ?

# In[ ]:


x_unit = X_test.iloc[375, :].copy()
x_unit['duration'] += 60

model.predict_proba([x_unit])

# Ici, la probabilit√© augmente de pr√®s de 70%, alors que la variable n'a augment√© que de 60 secondes. Ce qu'il faut regarder ici, ce sont les autres variables de l'observation.

# In[ ]:


X_test.iloc[375, :]

# Regardons le graphe de d√©pendance.

# In[ ]:


shap.dependence_plot("duration", shap_values, X_shap)

# Ici , l'interaction avec la variable `num_views_session` **est tr√®s forte** lorsque la dur√©e est tr√®s basse. Autrement dit, de petites dur√©es font fortement baisser la probabilit√© lorsqu'il n'y a que peu de vues dans une session.
# 
# Maintenant, essayons conjointement d'augmenter la valeur de la variable `num_views_session`.

# In[ ]:


x_unit = X_test.iloc[375, :].copy()
x_unit['duration'] += 10
x_unit['num_views_session'] += 10

model.predict_proba([x_unit])

# Dans ce contexte, la probabilit√© **reste tr√®s faible**. Ce test directionnel s'int√©resserait donc √† des observations avec de faibles dur√©es et peu de vues.
# 
# Prenons un autre exemple, cette fois-ci pour un utilisateur ayant une forte probabilit√© de finaliser son achat.

# In[ ]:


x_unit = X_test.loc[4720, :]
 
model.predict_proba([x_unit])

# In[ ]:


shap.force_plot(explainer.expected_value[0], shap_values[4720, :], x_unit, matplotlib=True)

# Retirons maintenant 60 secondes √† cette observation.

# In[ ]:


x_unit = X_test.loc[4720, :].copy()
x_unit['duration'] -= 60
    
model.predict_proba([x_unit])

# L'effet de la variable `duration` a beaucoup moins d'impact que pour l'observation pr√©c√©dente.
# 
# Ce qu'il faut retenir, c'est qu'il ne suffit pas de d√©finir un seuil limite d'√©cart de probabilit√© en appliquant une perturbation $\varepsilon$ sans √©tudier au pr√©alable l'observation qui va subir la transformation. Dans le premier exemple, la dur√©e √©tait tr√®s faible (seule 10 secondes), il √©tait donc logique sur la probabilit√© de finaliser l'achat soit tr√®s faible. En revanche, le fait de rajouter 60 secondes pour cette session peut cr√©er une observation que est pas ou tr√®s peu repr√©sent√©e dans l'√©chantillon : le mod√®le n'a rencontr√© que peu d'observations pr√©sentant ces caract√©ristiques.

# ### Tests unitaires du mod√®le
# 
# Au m√™me titre que les tests unitaires sont r√©alis√©s pour les fonctions de collecte et de transformation de donn√©es, les tests unitaires pour le mod√®le consistent √† v√©rifier que ce dernier pr√©dit la bonne r√©ponse pour des observations qui sont suppos√©es √™tre parfaitement classifi√©es.
# 
# Une m√©thode consiste √† calculer des **prototypes** : il s'agit d'observations qui *repr√©sentent le plus* les donn√©es. En d'autres termes, il s'agit d'un concept proches des centres de clusters form√©s par les observations. Et un algorithme non-supervis√© permettant de d√©tecter les prototypes est le **k-m√©do√Øde**, proche des k-moyennes dans son fonctionnement mais qui calcule le <a href="https://en.wikipedia.org/wiki/Medoid" target="_blank">m√©do√Øde</a>, point d'un cluster dont la distance avec tous les autres points est la plus petite.

# In[ ]:


!pip install scikit-learn-extra -q

# Lan√ßons un k-m√©do√Øde sur les observations de test.

# In[ ]:


from sklearn_extra.cluster import KMedoids

kmed = KMedoids(n_clusters=10)
kmed.fit(X_test)

# R√©cup√©rons les centres des clusters (les m√©do√Ødes) dans un DataFrame.

# In[ ]:


X_prototypes = pd.DataFrame(
    data=kmed.cluster_centers_,
    columns=X_test.columns
)
X_prototypes

# Chacune de ces observations repr√©sentent la moyenne d'une sous-population de l'√©chantillon. √âtonnamment, hormis la premi√®re observation, toutes les autres concernent des produits issus de la m√™me cat√©gorie.
# 
# Calculons les probabilit√© associ√©es.

# In[ ]:


model.predict_proba(kmed.cluster_centers_)

# L√†-aussi, √† part la premi√®re observation, toutes les autres sont pr√©dites dans la classe positive. La derni√®re observation est, quant-√†-elle, plus difficile √† quantifier du fait des deux probabilit√©s tr√®s proches.
# 
# Nous pourrions ainsi extraire plusieurs prototypes de ce DataFrame. Attention n√©anmoins, car ces donn√©es repr√©sentent uniquement un historique d'une journ√©e, alors qu'en pratique, celles qui seront utilis√©es pour calibrer le mod√®le repr√©sentent un historique de 7 jours.

# ## ‚úîÔ∏è Conclusion
# 
# Les tests de mod√®le sont plus difficiles √† construire, mais sont indispensables pour certains secteurs d'activit√©s o√π les pr√©dictions du mod√®le peuvent √™tre critiques.
# 
# - Nous avons vu les test pr√©-entra√Ænement pour s'assurer de la coh√©rence de la base d'apprentissage avant l'entra√Ænement.
# - Nous avons d√©taill√© plusieurs tests de mod√®les pour v√©rifier son comportement.
# 
# > ‚û°Ô∏è Lorsque ces tests sont r√©alis√©s avec succ√®s, il faut maintenant conserver le mod√®le quelque part pour y acc√©der ult√©rieurement : c'est le r√¥le de <b>MLflow</b>.
